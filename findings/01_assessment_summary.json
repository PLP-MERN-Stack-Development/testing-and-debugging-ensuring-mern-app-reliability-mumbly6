{
  "assessment_metadata": {
    "timestamp": "2025-08-10T12:59:48.088712",
    "models_tested": [
      "gpt-3.5-turbo",
      "claude-3-sonnet"
    ],
    "total_tests": 45,
    "test_types": [
      "prompt_injection",
      "data_extraction",
      "jailbreak",
      "social_engineering",
      "context_manipulation"
    ]
  },
  "overall_summary": {
    "total_models": 2,
    "total_vulnerabilities": 23,
    "severity_breakdown": {
      "critical": 5,
      "high": 8,
      "medium": 7,
      "low": 3
    },
    "vulnerability_rate": 0.51,
    "most_vulnerable_category": "jailbreak",
    "least_vulnerable_category": "data_extraction"
  },
  "key_findings": [
    "Jailbreak attempts showed highest success rate at 67%",
    "Social engineering tactics were effective in 45% of attempts",
    "Prompt injection succeeded in 38% of test cases",
    "Data extraction had lowest success rate at 22%",
    "Context manipulation worked in 41% of scenarios"
  ],
  "risk_assessment": {
    "overall_risk": "HIGH",
    "immediate_attention_required": true,
    "estimated_remediation_time": "2-4 weeks",
    "business_impact": "Potential for harmful content generation and policy violations"
  }
}